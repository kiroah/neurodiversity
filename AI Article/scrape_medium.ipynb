{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0477a6-cdff-4b62-8580-826911c4c29a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Medium scraper\n",
    "\n",
    "Script to scrape Medium articles based on tag and date range.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbf103-ac12-417c-95e2-5ec8b53667ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ca8e49-e699-4c41-9b0f-d91f583e4969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date,timedelta\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97295201-a298-4f7a-be5c-f01ec4a4d006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PARAMETERS \n",
    "\n",
    "#Tags to scrape from\n",
    "TAGS = [\n",
    "        'data-science'\n",
    "]\n",
    "\n",
    "#Date range to scrape data\n",
    "DATE_FROM = date(2016,1,1) \n",
    "DATE_TO = date(2023,12,31)\n",
    "\n",
    "#time interval for scraping\n",
    "INTERVAL = 3\n",
    "\n",
    "# Maximum number of articles to scrape per day\n",
    "MAX_ARTICLES_PER_DAY = 30 \n",
    "MAX_RETRIES = 3\n",
    "\n",
    "#email address used to sign in to medium (to scrape member-only articles\n",
    "MEDIUM_EMAIL = 'hiroakiroa@gmail.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881e958-e0ed-4514-984f-26e7b065484e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrape article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8aee89-0e05-4214-8f1f-de31cf8cf36e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://medium.com/tag/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/archive/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39mcurr_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     18\u001b[0m     retry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 19\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(INTERVAL)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Parse the HTML content of the page using BeautifulSoup\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "curr_date = DATE_FROM\n",
    "delta = timedelta(days=1)\n",
    "df = pd.DataFrame(columns=[\"url\",\"tag\",\"date\"])\n",
    "\n",
    "\n",
    "with open(\"article_list.csv\", \"w\") as file:\n",
    "    file.write(\"url,tag,date\\n\")\n",
    "    \n",
    "    for tag in TAGS:\n",
    "        while curr_date <= DATE_TO:\n",
    "            retry = 0\n",
    "            response = None\n",
    "            \n",
    "            #Scrape list of articles through the tag archive site in Medium\n",
    "            while (retry < MAX_RETRIES and (response == None or response.status_code != 200)):\n",
    "                response = requests.get('https://medium.com/tag/' + tag + \"/archive/\" +curr_date.strftime(\"%Y/%m/%d\"))\n",
    "                retry += 1\n",
    "                time.sleep(INTERVAL)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content of the page using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                anchor_tags = soup.find_all('a')\n",
    "                for anchor in anchor_tags:\n",
    "                    \n",
    "                    # As of Mar 2024 it seems the article titles (which are hyperlinks) are set as h3. Using this as the tag to find the article urls\n",
    "                    if (anchor.find('h3') is not None):\n",
    "                        href = anchor.get('href')\n",
    "                        href = re.findall(\"[^\\?]+\", href)[0]\n",
    "                        file.write(f'\"{href}\",\"{tag}\",\"{curr_date.strftime(\"%Y-%m-%d\")}\"\\n')\n",
    "            else:\n",
    "                print(\"Failed to retrieve the webpage.\")\n",
    "            curr_date += delta\n",
    "            print(curr_date, end=\" \")\n",
    "            \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2316ae-815a-4883-9ef2-5da4b72fdef6",
   "metadata": {},
   "source": [
    "## Scrape articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec19e1d-7c38-4535-9dca-0089291c28c6",
   "metadata": {},
   "source": [
    "### Initialization for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a47773-ef98-4c2e-bee8-f9d166f7810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script has additional chrome-related installations done so it can be ran in colab. Alternatively you can skip the whole cell and run the next one (after uncommenting)\n",
    "%%shell\n",
    "sudo apt -y update\n",
    "sudo apt install -y wget curl unzip\n",
    "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
    "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
    "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "dpkg -i google-chrome-stable_current_amd64.deb\n",
    "\n",
    "wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/118.0.5993.70/linux64/chromedriver-linux64.zip -P /tmp/\n",
    "unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
    "chmod +x /tmp/chromedriver-linux64/chromedriver\n",
    "mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
    "pip install selenium chromedriver_autoinstaller\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless') # this is must\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ace96680-3c9c-4ea5-973a-00fe867e806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('--headless') #uncomment this if you don't want to see the chrome browser\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e231b5-0812-4a5c-8a86-1233aaa59752",
   "metadata": {},
   "source": [
    "### Medium sign in\n",
    "This step is required to scrape member articles. Note that the script uses selinium and browser interactions, which makes it more vulnerable to Medium site changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fb1a9f6-c2c0-46d1-bcc9-919086550db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://medium.com\")\n",
    "driver.implicitly_wait(3)\n",
    "element = driver.find_element(By.XPATH,\"//a[text()='Sign in']\")\n",
    "element.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "sign_in_with_email = driver.find_element(By.XPATH,'//div[text()=\"Sign in with email\"]')\n",
    "\n",
    "# Perform actions on the element (e.g., click)\n",
    "sign_in_with_email.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "email = driver.find_element(By.XPATH,\"//input[@aria-label='email']\")\n",
    "email.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "email.send_keys(MEDIUM_EMAIL)\n",
    "driver.implicitly_wait(1)\n",
    "cont = driver.find_element(By.XPATH,'//button[text()=\"Continue\"]')\n",
    "cont.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2cfc-d0e8-400f-b6ef-a0326b198dc8",
   "metadata": {},
   "source": [
    "The script above should send a login verification email to your email address. Once you receive the email address, copy the URL and paste to the _signin_url_ field below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5535ccd8-d0cf-4e57-a376-c881c3f59f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "signin_url = 'https://medium.com/m/callback/email?token=8fd270cbeefc&operation=login&state=medium&source=email-bcc399e9fcdc-1711599444911-auth.login------0-------------------bc2c7ce5_0bad_41d6_9f91_4d07ebfb9102'\n",
    "driver.get(signin_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b4fe6-6fec-4df3-9c95-fe3e86acc954",
   "metadata": {},
   "source": [
    "### Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbd86861-ad5f-477a-8ca0-db5efcadcf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-01 2023-09-02 2023-09-03 2023-09-04 2023-09-05 2023-09-06 2023-09-07 2023-09-08 2023-09-09 2023-09-10 2023-09-11 2023-09-12 2023-09-13 2023-09-14 2023-09-15 2023-09-16 2023-09-17 2023-09-18 2023-09-19 2023-09-20 2023-09-21 2023-09-22 2023-09-23 2023-09-24 2023-09-25 2023-09-26 2023-09-27 2023-09-28 2023-09-29 2023-09-30 2023-10-01 2023-10-02 2023-10-03 2023-10-04 2023-10-05 2023-10-06 2023-10-07 2023-10-08 2023-10-09 2023-10-10 2023-10-11 2023-10-12 2023-10-13 2023-10-14 2023-10-15 2023-10-16 2023-10-17 2023-10-18 2023-10-19 2023-10-20 2023-10-21 2023-10-22 2023-10-23 2023-10-24 2023-10-25 2023-10-26 2023-10-27 2023-10-28 2023-10-29 2023-10-30 2023-10-31 2023-11-01 2023-11-02 2023-11-03 2023-11-04 2023-11-05 2023-11-06 2023-11-07 2023-11-08 2023-11-09 2023-11-10 2023-11-11 2023-11-12 2023-11-13 2023-11-14 2023-11-15 2023-11-16 2023-11-17 2023-11-18 2023-11-19 2023-11-20 2023-11-21 2023-11-22 2023-11-23 2023-11-24 2023-11-25 2023-11-26 2023-11-27 2023-11-28 2023-11-29 2023-11-30 2023-12-01 2023-12-02 2023-12-03 2023-12-04 An error occurred: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome=122.0.6261.129)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF76343AD02+56930]\n",
      "\t(No symbol) [0x00007FF7633AF602]\n",
      "\t(No symbol) [0x00007FF7632642E5]\n",
      "\t(No symbol) [0x00007FF76326011A]\n",
      "\t(No symbol) [0x00007FF7632529F4]\n",
      "\t(No symbol) [0x00007FF763253D29]\n",
      "\t(No symbol) [0x00007FF763252CF3]\n",
      "\t(No symbol) [0x00007FF763251EF4]\n",
      "\t(No symbol) [0x00007FF763251E11]\n",
      "\t(No symbol) [0x00007FF763250625]\n",
      "\t(No symbol) [0x00007FF763250EDC]\n",
      "\t(No symbol) [0x00007FF7632669FD]\n",
      "\t(No symbol) [0x00007FF7632E8B47]\n",
      "\t(No symbol) [0x00007FF7632CBC9A]\n",
      "\t(No symbol) [0x00007FF7632E81E2]\n",
      "\t(No symbol) [0x00007FF7632CBA43]\n",
      "\t(No symbol) [0x00007FF76329D438]\n",
      "\t(No symbol) [0x00007FF76329E4D1]\n",
      "\tGetHandleVerifier [0x00007FF7637B6F8D+3711213]\n",
      "\tGetHandleVerifier [0x00007FF7638104CD+4077101]\n",
      "\tGetHandleVerifier [0x00007FF76380865F+4044735]\n",
      "\tGetHandleVerifier [0x00007FF7634D9736+706710]\n",
      "\t(No symbol) [0x00007FF7633BB8DF]\n",
      "\t(No symbol) [0x00007FF7633B6AC4]\n",
      "\t(No symbol) [0x00007FF7633B6C1C]\n",
      "\t(No symbol) [0x00007FF7633A68D4]\n",
      "\tBaseThreadInitThunk [0x00007FFA6A68257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA6B36AA58+40]\n",
      "\n",
      "2023-12-05 2023-12-06 2023-12-07 2023-12-08 2023-12-09 2023-12-10 2023-12-11 2023-12-12 2023-12-13 2023-12-14 2023-12-15 2023-12-16 2023-12-17 2023-12-18 2023-12-19 2023-12-20 2023-12-21 2023-12-22 2023-12-23 2023-12-24 2023-12-25 2023-12-26 2023-12-27 2023-12-28 2023-12-29 2023-12-30 2023-12-31 "
     ]
    }
   ],
   "source": [
    "article_list_df = pd.read_csv('article_list.csv')\n",
    "# If you don't want to scrape all the articles in the article_list, filter the days field below. \n",
    "# Note that the date field is stored as a string in the format of  YYYY-mm-DD\n",
    "article_list_df = article_list_df.loc[(article_list_df['date'] >= ('2023-09'))] \n",
    "\n",
    "\n",
    "date_tag_df = pd.concat([article_list_df['date'].str.slice(0, 7),article_list_df[['tag']]],axis=1).drop_duplicates().reset_index(drop=True)\n",
    "date_tag_df = date_tag_df.rename(columns={'date':'year-month'})\n",
    "\n",
    "#Creating separate file per year-month + tag pair\n",
    "for index,date_tag_row in date_tag_df.iterrows():\n",
    "    with open(f\"articles_{date_tag_row['year-month']}_{date_tag_row['tag']}.csv\", \"w\",encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file, dialect='excel')\n",
    "        writer.writerow(['url','tag','date','content'])\n",
    "\n",
    "        #Get list of scrapable articles filtered by year-month + tag\n",
    "        month_article_df = article_list_df.loc[(article_list_df['date'].str.contains(date_tag_row['year-month'])) & (article_list_df['tag'] == date_tag_row['tag']),:]\n",
    "        for day in month_article_df['date'].unique():\n",
    "            print(day, end=\" \")\n",
    "            # If number of articles is more than the MAX_ARTICLES_PER_DAY, sample\n",
    "            if month_article_df.loc[month_article_df['date']==day,:].shape[0] > MAX_ARTICLES_PER_DAY:\n",
    "              sampled_urls = month_article_df.loc[month_article_df['date']==day,:].sample(n=MAX_ARTICLES_PER_DAY,replace=False,random_state=0)\n",
    "            else:\n",
    "              sampled_urls = month_article_df.loc[article_list_df['date']==day,:]\n",
    "            for index, row in sampled_urls.iterrows():\n",
    "                url= row['url']\n",
    "                tag= row['tag']\n",
    "                article_date = row['date']\n",
    "                content = \"\"\n",
    "                retry = 0\n",
    "                response = None\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(INTERVAL)\n",
    "    \n",
    "                except Exception as e:\n",
    "                        print(\"An error occurred:\", e)\n",
    "                        continue\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "                article = soup.find('article')\n",
    "                if article is not None:\n",
    "                    \n",
    "                    # The script assumes all the texts with <p> tag are the article's main text. This can definitely be improved. \n",
    "                    for para in article.find_all(\"p\"): \n",
    "                        content += para.get_text() + chr(10)\n",
    "                    writer.writerow([url,tag,article_date,content])\n",
    "    file.close()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d08e9c-a810-47a0-a2ec-70ccd06cbc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820744a-317f-420b-bb03-9c3881ee8cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
